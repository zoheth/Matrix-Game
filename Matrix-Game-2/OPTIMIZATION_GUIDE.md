# CausalWanModel ä¼˜åŒ–æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ ç³»ç»Ÿåœ°ä¼˜åŒ– CausalWanModelï¼Œæå‡ DiffusionModel_Forward çš„æ€§èƒ½ã€‚

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ€§èƒ½åˆ†æ](#æ€§èƒ½åˆ†æ)
3. [ä¼˜åŒ–ç­–ç•¥](#ä¼˜åŒ–ç­–ç•¥)
4. [æµ‹è¯•æµç¨‹](#æµ‹è¯•æµç¨‹)
5. [å¸¸è§é™·é˜±](#å¸¸è§é™·é˜±)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šå»ºç«‹æ€§èƒ½åŸºå‡†

```bash
cd /home/ykx/dev/Matrix-Game/Matrix-Game-2

# è¿è¡ŒåŸºå‡†æµ‹è¯•
./tests/run_tests.sh benchmark

# æŸ¥çœ‹åŸºå‡†ç»“æœ
cat tests/baseline_performance.txt
```

é¢„æœŸè¾“å‡ºï¼š
```
Original Model Baseline
Mean: XXX.XX ms
Median: XXX.XX ms
P95: XXX.XX ms
```

### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ­£ç¡®æ€§æµ‹è¯•

```bash
# ç¡®ä¿åŸå§‹æ¨¡å‹æµ‹è¯•é€šè¿‡
./tests/run_tests.sh correctness
```

### ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹ä¼˜åŒ–

ç¼–è¾‘ `wan/modules/optimized_causal_model.py`ï¼Œåœ¨ TODO æ ‡è®°å¤„å®ç°ä½ çš„ä¼˜åŒ–ã€‚

### ç¬¬å››æ­¥ï¼šéªŒè¯ä¼˜åŒ–

```bash
# è¿è¡Œå¯¹æ¯”æµ‹è¯•
./tests/run_tests.sh compare
```

## ğŸ“Š æ€§èƒ½åˆ†æ

æ ¹æ® profiling æ•°æ®ï¼Œä¸»è¦ç“¶é¢ˆåœ¨ `DiffusionModel_Forward` ([utils/wan_wrapper.py:160](utils/wan_wrapper.py#L160))ï¼š

### ç“¶é¢ˆåˆ†è§£

```
DiffusionModel_Forward (æ€»æ—¶é—´: ~XXX ms)
â”œâ”€â”€ Attention (30-40%)
â”‚   â”œâ”€â”€ FlexAttention computation
â”‚   â”œâ”€â”€ RoPE application
â”‚   â””â”€â”€ KV cache indexing
â”œâ”€â”€ LayerNorm (15-20%)
â”‚   â”œâ”€â”€ Self-attention norm
â”‚   â”œâ”€â”€ Cross-attention norm
â”‚   â””â”€â”€ FFN norm
â”œâ”€â”€ FFN (20-25%)
â”‚   â””â”€â”€ Linear + GeLU + Linear
â”œâ”€â”€ KV Cache Update (10-15%)
â”‚   â”œâ”€â”€ Dynamic indexing
â”‚   â”œâ”€â”€ Memory copy
â”‚   â””â”€â”€ Index calculation
â””â”€â”€ Misc operations (10-15%)
    â”œâ”€â”€ Embeddings
    â”œâ”€â”€ Time projection
    â””â”€â”€ Unpatchify
```

### å°ç®—å­å¼€é”€

åŸå§‹å®ç°ä¸­å­˜åœ¨å¤§é‡å°ç®—å­ï¼š
- `unflatten` / `flatten` é¢‘ç¹è°ƒç”¨
- å¤šæ¬¡ `to()` / `type_as()` è½¬æ¢
- åŠ¨æ€ shape è®¡ç®—
- é¢‘ç¹çš„ CPU-GPU åŒæ­¥

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### ä¼˜å…ˆçº§ 1ï¼šä½æˆæœ¬é«˜æ”¶ç›Š (1-2å¤©)

#### 1.1 ç®—å­èåˆ

**ç›®æ ‡æ–‡ä»¶**: `wan/modules/optimized_causal_model.py`

```python
# èåˆ LayerNorm + Linear
class FusedLayerNormLinear(nn.Module):
    def forward(self, x):
        # å•æ¬¡ kernel è°ƒç”¨
        return F.linear(F.layer_norm(x, ...), self.weight, self.bias)
```

**é¢„æœŸæ”¶ç›Š**: 5-10% å»¶è¿Ÿé™ä½

#### 1.2 å‡å°‘ç±»å‹è½¬æ¢

```python
# åä¾‹å­ï¼ˆå½“å‰ä»£ç ï¼‰
x = x.to(dtype)
y = y.type_as(x)
z = z.to(device)

# å¥½ä¾‹å­
# åœ¨åˆå§‹åŒ–æ—¶ç»Ÿä¸€è®¾ç½®ï¼Œé¿å…è¿è¡Œæ—¶è½¬æ¢
```

**é¢„æœŸæ”¶ç›Š**: 3-5% å»¶è¿Ÿé™ä½

#### 1.3 Torch.compile å±€éƒ¨ç¼–è¯‘

```python
# ç¼–è¯‘æ—¶é—´åµŒå…¥
self.time_embedding = torch.compile(
    self.time_embedding,
    mode="reduce-overhead"
)

# ç¼–è¯‘ FFN
for block in self.blocks:
    block.ffn = torch.compile(block.ffn, mode="reduce-overhead")
```

**é¢„æœŸæ”¶ç›Š**: 10-15% å»¶è¿Ÿé™ä½

### ä¼˜å…ˆçº§ 2ï¼šä¸­ç­‰æˆæœ¬ä¸­ç­‰æ”¶ç›Š (3-5å¤©)

#### 2.1 é™æ€ KV Cache

**é—®é¢˜**ï¼šå½“å‰ KV cache ä½¿ç”¨åŠ¨æ€ç´¢å¼•ï¼Œå¯¼è‡´ï¼š
- æ— æ³•ä½¿ç”¨ CUDA Graph
- é¢‘ç¹çš„ç´¢å¼•è®¡ç®—å¼€é”€

**è§£å†³æ–¹æ¡ˆ**ï¼šç¯å½¢ç¼“å†²åŒº

```python
class RingBufferKVCache:
    def __init__(self, max_size, ...):
        # é¢„åˆ†é…å›ºå®šå¤§å° buffer
        self.buffer = torch.zeros(max_size, ...)
        self.head = 0  # å†™å…¥ä½ç½®

    def update(self, new_kv):
        # å›ºå®šä½ç½®å†™å…¥ï¼Œé¿å…åŠ¨æ€ç´¢å¼•
        size = new_kv.shape[0]
        self.buffer[self.head:self.head+size] = new_kv
        self.head = (self.head + size) % self.max_size
```

**é¢„æœŸæ”¶ç›Š**: 15-20% å»¶è¿Ÿé™ä½ + æ”¯æŒ CUDA Graph

#### 2.2 FlashAttention 3

æ›¿æ¢ FlexAttentionï¼š

```python
from flash_attn import flash_attn_func

class OptimizedAttention(nn.Module):
    def forward(self, q, k, v, ...):
        return flash_attn_func(q, k, v, causal=True)
```

**é¢„æœŸæ”¶ç›Š**: 20-30% Attention éƒ¨åˆ†åŠ é€Ÿ

### ä¼˜å…ˆçº§ 3ï¼šé«˜æˆæœ¬é«˜æ”¶ç›Š (1-2å‘¨)

#### 3.1 Triton è‡ªå®šä¹‰ Kernel

é’ˆå¯¹ç‰¹å®šæ¨¡å¼æ‰‹å†™èåˆ kernelï¼š

```python
import triton
import triton.language as tl

@triton.jit
def fused_attn_norm_ffn_kernel(...):
    # èåˆ Attention + Norm + FFN
    pass
```

**é¢„æœŸæ”¶ç›Š**: 30-40% æ•´ä½“åŠ é€Ÿ

#### 3.2 CUDA Graph

å®Œæˆé™æ€ KV cache åï¼Œå°è£…æ•´ä¸ªå‰å‘ä¼ æ’­ï¼š

```python
class CUDAGraphWrapper:
    def __init__(self, model):
        self.graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.graph):
            # å½•åˆ¶è®¡ç®—å›¾
            self.static_output = model(self.static_input, ...)

    def __call__(self, input):
        # æ›´æ–°è¾“å…¥ï¼Œé‡æ”¾å›¾
        self.static_input.copy_(input)
        self.graph.replay()
        return self.static_output
```

**é¢„æœŸæ”¶ç›Š**: 2-3x æ•´ä½“åŠ é€Ÿï¼ˆæœ€å¤§æ”¶ç›Šï¼‰

## ğŸ§ª æµ‹è¯•æµç¨‹

### æ­£ç¡®æ€§éªŒè¯

```bash
# æ¯æ¬¡ä¿®æ”¹åè¿è¡Œ
pytest tests/test_causal_wan_model.py::TestCausalWanModel::test_model_comparison -v -s
```

**æ¥å—æ ‡å‡†**ï¼š
- Max difference < 1e-2 (bfloat16)
- Mean difference < 1e-3

### æ€§èƒ½éªŒè¯

```bash
# å¯¹æ¯”åŸºå‡†
./tests/run_tests.sh benchmark
```

**è®°å½•æ”¶ç›Š**ï¼š
```
ä¼˜åŒ–ç‰ˆæœ¬ vs åŸºå‡†
- å»¶è¿Ÿé™ä½: XX%
- P95 æ”¹å–„: XX%
```

### å®Œæ•´æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./tests/run_tests.sh all
```

## âš ï¸ å¸¸è§é™·é˜±

### é™·é˜± 1ï¼šè¿‡æ—©ä¼˜åŒ–

âŒ **é”™è¯¯åšæ³•**ï¼š
```python
# ä¸€æ¬¡æ€§é‡å†™æ‰€æœ‰ä»£ç 
class OptimizedCausalWanModel:
    def forward(self, ...):
        # 1000 è¡Œå…¨æ–°å®ç°
```

âœ… **æ­£ç¡®åšæ³•**ï¼š
```python
# é€æ­¥ä¼˜åŒ–ï¼Œæ¯æ­¥æµ‹è¯•
def _forward_inference(self, ...):
    # æ­¥éª¤1ï¼šä¼˜åŒ– embedding (æµ‹è¯•é€šè¿‡)
    x = self._optimized_embed(x)

    # æ­¥éª¤2ï¼šä¼˜åŒ– attention (æµ‹è¯•é€šè¿‡)
    for block in self.blocks:
        x = self._optimized_block(block, x)

    # æ­¥éª¤3ï¼šä¼˜åŒ– output (æµ‹è¯•é€šè¿‡)
    return self._optimized_output(x)
```

### é™·é˜± 2ï¼šå¿½è§†æ•°å€¼ç²¾åº¦

ä½¿ç”¨ bfloat16 æ—¶è¦æ³¨æ„ï¼š
- Softmax å®¹æ˜“æº¢å‡º
- å°æ•°ç´¯åŠ è¯¯å·®

**è§£å†³æ–¹æ¡ˆ**ï¼šå…³é”®è·¯å¾„ä½¿ç”¨ float32
```python
# Attention ä¸­ä½¿ç”¨æ›´é«˜ç²¾åº¦
attn_weights = F.softmax(scores.float(), dim=-1).to(dtype)
```

### é™·é˜± 3ï¼šCUDA Graph çš„é™åˆ¶

CUDA Graph è¦æ±‚ï¼š
- âœ… é™æ€ shape
- âœ… å›ºå®šæ§åˆ¶æµ
- âœ… æ—  CPU åŒæ­¥
- âŒ ä¸æ”¯æŒåŠ¨æ€ç´¢å¼•
- âŒ ä¸æ”¯æŒ `if`/`for` åŠ¨æ€åˆ†æ”¯

**æ£€æŸ¥æ–¹æ³•**ï¼š
```python
# å¦‚æœçœ‹åˆ°è¿™äº›ï¼ŒCUDA Graph ä¸å¯ç”¨
if kv_cache is not None:  # åŠ¨æ€åˆ†æ”¯
    ...

kv_cache["k"][:, start:end]  # åŠ¨æ€ç´¢å¼• (start/end æ˜¯å˜é‡)
```

### é™·é˜± 4ï¼šç¼–è¯‘å¼€é”€

torch.compile é¦–æ¬¡è¿è¡Œå¾ˆæ…¢ï¼ˆç¼–è¯‘æ—¶é—´ï¼‰ï¼š
- é¢„çƒ­ï¼š3-10 æ¬¡è¿­ä»£
- ç¼–è¯‘ç¼“å­˜ï¼šä¿å­˜åˆ°ç£ç›˜

```python
# é¦–æ¬¡ç¼–è¯‘åä¿å­˜
compiled_model = torch.compile(model)
torch.save(compiled_model, "compiled_model.pt")

# åç»­ç›´æ¥åŠ è½½
compiled_model = torch.load("compiled_model.pt")
```

## ğŸ“ˆ ä¼˜åŒ–è·¯çº¿å›¾

### Week 1: åŸºç¡€ä¼˜åŒ–
- [ ] å»ºç«‹æµ‹è¯•åŸºå‡†
- [ ] å®ç°ç®—å­èåˆ (LayerNorm + Linear)
- [ ] å‡å°‘ç±»å‹è½¬æ¢
- [ ] å±€éƒ¨ torch.compile

**ç›®æ ‡**: 15-20% å»¶è¿Ÿé™ä½

### Week 2: ä¸­çº§ä¼˜åŒ–
- [ ] å®ç°é™æ€ KV Cache (ç¯å½¢ç¼“å†²åŒº)
- [ ] é›†æˆ FlashAttention 3
- [ ] ä¼˜åŒ– RoPE è®¡ç®—

**ç›®æ ‡**: ç´¯è®¡ 30-35% å»¶è¿Ÿé™ä½

### Week 3-4: é«˜çº§ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
- [ ] æ‰‹å†™ Triton kernel (Attention + Norm + FFN)
- [ ] CUDA Graph å°è£…
- [ ] å¤šæµå¹¶è¡Œï¼ˆå¦‚æœé€‚ç”¨ï¼‰

**ç›®æ ‡**: ç´¯è®¡ 50-70% å»¶è¿Ÿé™ä½ï¼Œæˆ– 2-3x åŠ é€Ÿ

## ğŸ”§ è°ƒè¯•æŠ€å·§

### æ€§èƒ½åˆ†æ

```bash
# ä½¿ç”¨ PyTorch Profiler
python inference.py --enable_profile
# æŸ¥çœ‹ Chrome trace
```

### æ•°å€¼è°ƒè¯•

```python
# åœ¨ä¼˜åŒ–ä»£ç ä¸­æ’å…¥æ£€æŸ¥ç‚¹
def forward(self, x):
    x_orig = self._original_forward(x)
    x_opt = self._optimized_forward(x)

    diff = (x_orig - x_opt).abs().max()
    if diff > 1e-2:
        print(f"Warning: numerical diff = {diff}")

    return x_opt
```

### CUDA Graph è°ƒè¯•

```python
# æ£€æŸ¥æ˜¯å¦å¯ä»¥ç”¨ CUDA Graph
torch.cuda.make_graphed_callables(model, sample_args)
# å¦‚æœæŠ¥é”™ï¼Œè¯´æ˜æœ‰ä¸å…¼å®¹çš„æ“ä½œ
```

## ğŸ“š å‚è€ƒèµ„æº

- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [CUDA Graphs Tutorial](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)
- [Triton Documentation](https://triton-lang.org/)

## ğŸ’¡ æç¤º

1. **å¢é‡å¼€å‘**ï¼šæ¯æ¬¡ä¼˜åŒ–ä¸€ä¸ªæ¨¡å—ï¼Œç«‹å³æµ‹è¯•
2. **æ€§èƒ½ç›‘æ§**ï¼šä½¿ç”¨ `PerformanceMonitor` è¿½è¸ªæ¯ä¸ªæ¨¡å—çš„è€—æ—¶
3. **ç‰ˆæœ¬æ§åˆ¶**ï¼šæ¯ä¸ªä¼˜åŒ–é˜¶æ®µæäº¤ git
4. **æ–‡æ¡£è®°å½•**ï¼šè®°å½•æ¯æ¬¡ä¼˜åŒ–çš„æ”¶ç›Šå’Œé—®é¢˜

---

**Good luck with your optimization!** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œå‚è€ƒ `tests/README.md` æˆ–æŸ¥çœ‹æµ‹è¯•ä»£ç ç¤ºä¾‹ã€‚
